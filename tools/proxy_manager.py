import os, sys, json, asyncio, cloudscraper, random

# -------------------- æ–‡ä»¶è·¯å¾„ --------------------
def get_base_path():
    if getattr(sys, 'frozen', False):
        return os.path.dirname(sys.executable)
    return os.path.dirname(os.path.abspath(__file__))

PROXY_JSON_FILE = os.path.join(get_base_path(), "stable_proxies.json")

if not os.path.exists(PROXY_JSON_FILE):
    os.makedirs(os.path.dirname(PROXY_JSON_FILE), exist_ok=True)
    with open(PROXY_JSON_FILE, "w", encoding="utf-8") as f:
        json.dump({}, f)

# -------------------- æ–‡ä»¶æ“ä½œ --------------------
def load_proxy_file():
    if os.path.exists(PROXY_JSON_FILE):
        with open(PROXY_JSON_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_proxy_file(proxy_dict):
    with open(PROXY_JSON_FILE, "w", encoding="utf-8") as f:
        json.dump(proxy_dict, f, ensure_ascii=False, indent=2)

# -------------------- Cloudscraper ä»£ç†æ£€æµ‹ --------------------
def test_proxy_cloudscraper(proxy: str, test_url: str, timeout=20, log_func=print, verbose=True):
    result = {"http": False, "https": False}
    scraper = cloudscraper.create_scraper(
        browser={"browser": "chrome", "platform": "windows", "desktop": True}
    )
    if verbose: log_func(f"\nğŸ” æµ‹è¯•ä»£ç†: {proxy}")

    for scheme in ("https", "http"):
        proxy_url = f"{scheme}://{proxy}"
        proxies = {"http": proxy_url, "https": proxy_url}
        try:
            if verbose: log_func(f"   â–¶ è®¿é—® {test_url} via {proxy_url}")
            resp = scraper.get(test_url, proxies=proxies, timeout=timeout)
            if resp.status_code == 200:
                result[scheme] = True
                if verbose: log_func(f"   âœ… å¯ç”¨ä»£ç† ({scheme})")
        except Exception as e:
            if verbose: log_func(f"   âŒ å¤±è´¥ ({scheme}) {type(e).__name__}")
    return result

# -------------------- æ‰¹é‡æµ‹è¯• --------------------
async def test_proxies_batch(proxy_dict, test_url, batch_size=3, log_func=print, verbose=True, max_keep=50):
    keys = list(proxy_dict.keys())
    log_func(f"ğŸ§ª å¼€å§‹æ‰¹é‡æµ‹è¯• {len(keys)} ä¸ªä»£ç†...")

    for i in range(0, len(keys), batch_size):
        batch = keys[i:i+batch_size]
        log_func(f"ğŸ”¹ æ­£åœ¨æµ‹è¯•æ‰¹æ¬¡ {i//batch_size+1} ...")

        total_proxies = len(proxy_dict)
        fail_threshold = 4 if total_proxies > 30 else 3 if total_proxies > 15 else 2 if total_proxies > 5 else 1

        tasks = [asyncio.to_thread(test_proxy_cloudscraper, p, test_url, 20, log_func, verbose) for p in batch]
        results = await asyncio.gather(*tasks)

        for proxy, res in zip(batch, results):
            for scheme in ("http", "https"):
                if res[scheme]:
                    proxy_dict[proxy][f"{scheme}_score"] = proxy_dict[proxy].get(f"{scheme}_score", 0) + 1
                    proxy_dict[proxy][f"{scheme}_fail"] = 0
                else:
                    proxy_dict[proxy][f"{scheme}_score"] = max(-5, proxy_dict[proxy].get(f"{scheme}_score", 0) - 1)
                    proxy_dict[proxy][f"{scheme}_fail"] = proxy_dict[proxy].get(f"{scheme}_fail", 0) + 1

        for p in list(proxy_dict.keys()):
            if proxy_dict[p]["http_fail"] >= fail_threshold and proxy_dict[p]["https_fail"] >= fail_threshold:
                proxy_dict[p]["http_score"] //= 2
                proxy_dict[p]["https_score"] //= 2
                proxy_dict[p]["http_fail"] = 0
                proxy_dict[p]["https_fail"] = 0
                if verbose: log_func(f"âš ï¸ ä»£ç† {p} è¿ç»­å¤±è´¥ï¼Œå¥åº·å€¼å‡åŠ")
            if proxy_dict[p]["http_score"] <= 0 and proxy_dict[p]["https_score"] <= 0:
                del proxy_dict[p]
                if verbose: log_func(f"âŒ ç§»é™¤ä½å¥åº·ä»£ç†: {p}")

        if len(proxy_dict) > max_keep:
            sorted_proxies = sorted(proxy_dict.items(), key=lambda x: max(x[1]["http_score"], x[1]["https_score"]), reverse=True)
            proxy_dict = dict(sorted_proxies[:max_keep])
            log_func(f"âš ï¸ ä»£ç†æ± è£å‰ªè‡³ {max_keep} ä¸ªé«˜å¥åº·ä»£ç†...")

        save_proxy_file(proxy_dict)
        await asyncio.sleep(0.5)

    log_func(f"âœ… æ‰¹é‡æµ‹è¯•å®Œæˆï¼Œå¯ç”¨ä»£ç† {len(proxy_dict)} ä¸ª")
    return proxy_dict

# -------------------- å…è´¹ä»£ç†æŠ“å– --------------------
async def fetch_free_proxies(log_func=print, rounds=3, test_url=None, verbose=False, max_keep=50):
    proxy_dict = load_proxy_file()
    log_func(f"ğŸ“‚ åŠ è½½æœ¬åœ°ä»£ç†æ± : {len(proxy_dict)} ä¸ª")

    proxy_dict = await test_proxies_batch(proxy_dict, test_url, log_func=log_func, verbose=verbose, max_keep=max_keep)
    log_func(f"âœ… æœ¬åœ°ä»£ç†æ£€æµ‹å®Œæˆï¼Œå¯ç”¨ä»£ç†: {len(proxy_dict)}")

    PROXY_APIS = [
        "https://proxy.scdn.io/api/get_proxy.php?protocol=https&count=20",
        "https://www.proxy-list.download/api/v1/get?type=https",
        "https://api.getproxylist.com/proxy?protocol=https"
    ]
    scraper = cloudscraper.create_scraper()

    for i in range(rounds):
        log_func(f"ğŸŒ ç¬¬ {i+1} æ¬¡æŠ“å–ä»£ç†...")
        for api_url in PROXY_APIS:
            try:
                resp = await asyncio.to_thread(scraper.get, api_url, timeout=15)
                proxies = []

                try:
                    response = json.loads(resp.text)
                    if "data" in response and "proxies" in response["data"]:
                        proxies = response["data"]["proxies"]
                    elif isinstance(response, list):
                        proxies = response
                except Exception:
                    proxies = resp.text.splitlines()

                proxies = [p.strip() for p in proxies if p.strip()]
                log_func(f"è·å–åˆ° {len(proxies)} ä¸ªä»£ç†")

                for p in proxies:
                    if p not in proxy_dict:
                        proxy_dict[p] = {"http_score":0, "https_score":0, "http_fail":0, "https_fail":0}
                    else:
                        proxy_dict[p]["http_score"] = max(proxy_dict[p]["http_score"], 1)
                        proxy_dict[p]["https_score"] = max(proxy_dict[p]["https_score"], 1)
                        proxy_dict[p]["http_fail"] = 0
                        proxy_dict[p]["https_fail"] = 0

                proxy_dict = await test_proxies_batch(proxy_dict, test_url, log_func=log_func, verbose=verbose, max_keep=max_keep)

            except Exception as e:
                log_func(f"âŒ æŠ“å–å¼‚å¸¸: {api_url} {e}")

        await asyncio.sleep(2)

    save_proxy_file(proxy_dict)
    log_func(f"âœ… ä»£ç†æ± æ›´æ–°å®Œæˆï¼Œå…± {len(proxy_dict)} ä¸ª")
    return proxy_dict

# -------------------- è·å–å·¥ä½œä»£ç† --------------------
cached_working_proxies = {}

async def get_working_proxy(url, log_func=print, top_n=20):
    scheme = "https" if url.startswith("https") else "http"
    global cached_working_proxies

    if not cached_working_proxies:
        proxy_dict = load_proxy_file()
        if not proxy_dict:
            log_func("âš ï¸ ä»£ç†æ± ä¸ºç©º")
            return None
        cached_working_proxies = proxy_dict

    available_proxies = [p for p, info in cached_working_proxies.items() if info[f"{scheme}_score"] > 0]
    if not available_proxies:
        fallback_scheme = "http" if scheme == "https" else "https"
        available_proxies = [p for p, info in cached_working_proxies.items() if info[f"{fallback_scheme}_score"] > 0]
        if available_proxies:
            log_func(f"âš ï¸ {scheme.upper()} ä»£ç†ä¸è¶³ï¼Œä½¿ç”¨ {fallback_scheme.upper()} fallback")
            scheme = fallback_scheme
        else:
            log_func("âš ï¸ æ²¡æœ‰å¯ç”¨ä»£ç†")
            return None

    sorted_proxies = sorted(
        available_proxies,
        key=lambda p: cached_working_proxies[p][f"{scheme}_score"],
        reverse=True
    )
    top_proxies = sorted_proxies[:min(top_n, len(sorted_proxies))]
    weights = [cached_working_proxies[p][f"{scheme}_score"]**2 for p in top_proxies]
    chosen = random.choices(top_proxies, weights=weights, k=1)[0]

    log_func(f"ğŸŒ ä½¿ç”¨ä»£ç†: {chosen} ({scheme}) [å¥åº·å€¼: {cached_working_proxies[chosen][f'{scheme}_score']}]")
    return chosen

def remove_bad_proxy(proxy, log_func=print):
    global cached_working_proxies
    if proxy in cached_working_proxies:
        del cached_working_proxies[proxy]
        log_func(f"âŒ ç«‹å³ç§»é™¤åä»£ç†: {proxy}")
        save_proxy_file(cached_working_proxies)

async def run_proxy_check(url_to_test, log_func=print, rounds=5, verbose=True):
    log_func("ğŸš€ å¼€å§‹å®Œæ•´ä»£ç†æ£€æµ‹æµç¨‹...")
    proxy_dict = await fetch_free_proxies(log_func=log_func, rounds=rounds, test_url=url_to_test, verbose=verbose)
    global cached_working_proxies
    cached_working_proxies = proxy_dict
    log_func(f"âœ… æ›´æ–°åå¯ç”¨ä»£ç†æ•°é‡: {len(proxy_dict)}")
    return proxy_dict

def check_proxies(url_to_test="https://httpbin.org/get", log_func=None, rounds=5, verbose=False):
    return asyncio.run(run_proxy_check(url_to_test=url_to_test, log_func=log_func, rounds=rounds, verbose=verbose))

def print_proxy_health(log_func=print):
    global cached_working_proxies
    if not cached_working_proxies:
        log_func("âš ï¸ å½“å‰æ²¡æœ‰ç¼“å­˜ä»£ç†")
        return

    sorted_proxies = sorted(cached_working_proxies.items(), key=lambda x: max(x[1]["http_score"], x[1]["https_score"]), reverse=True)
    log_func("\nğŸ“Š å½“å‰ä»£ç†å¥åº·å€¼:")
    log_func("ä»£ç†åœ°å€\t\thttp_score\thttps_score\thttp_fail\thttps_fail")
    log_func("-"*50)
    for proxy, info in sorted_proxies:
        log_func(f"{proxy}\t{info['http_score']}\t{info['https_score']}\t{info['http_fail']}\t{info['https_fail']}")
    log_func(f"âœ… æ€»ä»£ç†æ•°: {len(sorted_proxies)}\n")

if __name__ == "__main__":
    def log_print(msg): print(msg)

    test_url = "https://httpbin.org/get"

    log_print("ğŸ”¹ æµ‹è¯•ä»£ç†æ£€æµ‹åŠŸèƒ½å¼€å§‹ ğŸ”¹")
    log_print("âš ï¸ æ³¨æ„ï¼šæµ‹è¯•è¿‡ç¨‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    stable_proxies = check_proxies(url_to_test=test_url, log_func=log_print, rounds=3, verbose=True)

    print_proxy_health(log_print)

    log_print("\nâœ… å¯ç”¨ä»£ç†åˆ—è¡¨:")
    for p in stable_proxies:
        print(p)

    log_print("ğŸ”¹ æµ‹è¯•å®Œæˆ ğŸ”¹")
